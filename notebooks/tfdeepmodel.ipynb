{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/DeepLearningModelDeployment/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd /root/DeepLearningModelDeployment/notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.7/site-packages (21.1.2)\n",
      "Collecting pip\n",
      "  Using cached pip-22.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.1.2\n",
      "    Uninstalling pip-21.1.2:\n",
      "      Successfully uninstalled pip-21.1.2\n",
      "Successfully installed pip-22.1\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip \n",
    "!pip install -q sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'tf-deep-imdb-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n"
     ]
    }
   ],
   "source": [
    "# Load imdb data and assign to train and test\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 400\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create folder structure and copy train and test data to npy files.\n",
    "npy files contains an array saved in the NumPy (NPY) file format. \n",
    "NPY files store all the information required to reconstruct an array on any computer, \n",
    "which includes dtype and shape information\n",
    "\"\"\"\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'imdb_data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), 'imdb_data/train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), 'imdb_data/test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "csv_test_dir = os.path.join(os.getcwd(), 'imdb_data/csv-test')\n",
    "os.makedirs(csv_test_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(train_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(train_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(test_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(test_dir, 'y_test.npy'), y_test)\n",
    "np.savetxt(os.path.join(csv_test_dir, 'csv-test.csv'), \n",
    "           np.array(x_test[:100], dtype=np.int32), fmt='%d', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload train and test data into default s3 bucket\n",
    "\n",
    "traindata_s3_prefix = f'{prefix}/imdb_data/train'\n",
    "testdata_s3_prefix = f'{prefix}/imdb_data/test'\n",
    "\n",
    "train_s3 = sess.upload_data(path='./imdb_data/train/', key_prefix=traindata_s3_prefix)\n",
    "test_s3 = sess.upload_data(path='./imdb_data/test/', key_prefix=testdata_s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘code’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/smdp_tensorflow_sentiment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/smdp_tensorflow_sentiment.py\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "import argparse\n",
    "import codecs\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import smdistributed.dataparallel.tensorflow as sdp\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 400\n",
    "embedding_dims = 300\n",
    "filters = 256\n",
    "kernel_size = 3\n",
    "hidden_dims = 256\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--drop_out_rate', type=float, default=0.2)\n",
    "\n",
    "    # data directories\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "\n",
    "    # model directory /opt/ml/model default set by SageMaker\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "def get_train_data(train_dir, batch_size):\n",
    "    x_train = np.load(os.path.join(train_dir, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(train_dir, 'y_train.npy'))\n",
    "    print(f'x train {x_train.shape} y train {y_train.shape}')\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_test_data(test_dir):\n",
    "    x_test = np.load(os.path.join(test_dir, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join(test_dir, 'y_test.npy'))\n",
    "    print(f'x test {x_test.shape} y test {y_test.shape}')\n",
    "\n",
    "    return x_test, y_test\n",
    "\n",
    "\n",
    "def get_model(args):\n",
    "    embedding_layer = tf.keras.layers.Embedding(max_features,\n",
    "                                                embedding_dims,\n",
    "                                                input_length=maxlen)\n",
    "\n",
    "    sequence_input = tf.keras.Input(shape=(maxlen,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = tf.keras.layers.Dropout(args.drop_out_rate)(embedded_sequences)\n",
    "    x = tf.keras.layers.Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)(x)\n",
    "    x = tf.keras.layers.MaxPooling1D()(x)\n",
    "    x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "    x = tf.keras.layers.Dense(hidden_dims, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(args.drop_out_rate)(x)\n",
    "    preds = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(sequence_input, preds)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(train_dataset, args):\n",
    "    model = get_model(args)\n",
    "    \n",
    "    loss = tf.losses.BinaryCrossentropy(name = 'binary_crossentropy')\n",
    "    acc = tf.metrics.BinaryAccuracy(name = 'accuracy')\n",
    "    optimizer = tf.optimizers.Adam(learning_rate = args.learning_rate)\n",
    "    \n",
    "    @tf.function\n",
    "    def training_step(x_train, y_train, first_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            probs = model(x_train, training=True)\n",
    "            loss_value = loss(y_train, probs)\n",
    "            acc_value = acc(y_train, probs)\n",
    "\n",
    "        # SMDataParallel: Wrap tf.GradientTape with SMDataParallel's DistributedGradientTape\n",
    "        tape = sdp.DistributedGradientTape(tape, sparse_as_dense = True)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if first_batch:\n",
    "            print('first batch')\n",
    "            # SMDataParallel: Broadcast model and optimizer variables\n",
    "            sdp.broadcast_variables(model.variables, root_rank=0)\n",
    "            sdp.broadcast_variables(optimizer.variables(), root_rank=0)\n",
    "\n",
    "        # SMDataParallel: all_reduce call\n",
    "        loss_value = sdp.oob_allreduce(loss_value)  # Average the loss across workers\n",
    "        acc_value = sdp.oob_allreduce(acc_value)\n",
    " \n",
    "        return loss_value, acc_value\n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        for batch, (x_train, y_train) in enumerate(train_dataset.take(len(train_dataset)//sdp.size())):\n",
    "            is_first_batch = (epoch == 0) and (batch == 0)\n",
    "            loss_value, acc_value = training_step(x_train, y_train, is_first_batch)\n",
    "\n",
    "            if batch % 10 == 0 and sdp.rank() == 0:\n",
    "                print('Epoch #%d, Step #%d\\tLoss: %.6f, Acc: %.6f (batch_size=%d)' % (epoch, batch, loss_value, acc_value, len(y_train)))\n",
    "\n",
    "    # SMDataParallel: Save checkpoints only from master node.\n",
    "    if sdp.rank() == 0:\n",
    "        model.save(os.path.join(args.model_dir, '1'))\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args, _ = parse_args()\n",
    "\n",
    "    # initialize sagemaker data parallel (dist)\n",
    "    sdp.init()\n",
    "\n",
    "    # ping each GPU to a single smdistributed.dataparallel process with local_rank\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    if gpus:\n",
    "        tf.config.experimental.set_visible_devices(gpus[sdp.local_rank()], 'GPU')\n",
    "        \n",
    "    # scale the learning rate by number of workers\n",
    "    print('sdp.size() = %s' % sdp.size())\n",
    "    args.learning_rate = args.learning_rate * sdp.size()\n",
    "    \n",
    "    train_dataset = get_train_data(args.train, args.batch_size)\n",
    "\n",
    "    train(train_dataset, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from botocore.exceptions import ClientError\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "experiment_name = 'imdb-experiment-tfdeepmode2test'\n",
    "\n",
    "try:\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=experiment_name, \n",
    "        description='Training a sentiment classification model using imdb dataset.')\n",
    "except ClientError as e:\n",
    "    print(f'{experiment_name} experiment already exists! Reusing the existing experiment.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://sagemaker-us-east-1-104877823522/tf-deep-imdb-model/imdb_data/train', 'test': 's3://sagemaker-us-east-1-104877823522/tf-deep-imdb-model/imdb_data/test'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "jobname = f'imdb-tf-deepmodel-job-{exp_datetime}'\n",
    "\n",
    "s3_output_location = f's3://{bucket}/{prefix}/{jobname}'\n",
    "code_dir = f's3://{bucket}/{prefix}/{jobname}'\n",
    "\n",
    "train_instance_type = 'ml.p3.16xlarge'\n",
    "hyperparameters = {'epochs': 30, 'batch_size': 512, \n",
    "                   'learning_rate': 0.001, 'drop_out_rate': 0.2}\n",
    "distribution = {'smdistributed': {'dataparallel': {'enabled': True}}}\n",
    "\n",
    "estimator = TensorFlow(source_dir='code',\n",
    "                       entry_point='smdp_tensorflow_sentiment.py',\n",
    "                       output_path=s3_output_location,\n",
    "                       code_location=code_dir,\n",
    "                       instance_type=train_instance_type,\n",
    "                       instance_count=1,\n",
    "                       enable_sagemaker_metrics=True,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       sagemaker_session=sess,\n",
    "                       role=role,\n",
    "                       framework_version='2.4',\n",
    "                       py_version='py37', \n",
    "                       distribution=distribution)\n",
    "\n",
    "data_channels = {'train':train_s3, 'test': test_s3}\n",
    "print(data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: imdb-tf-deepmodel-job-2022-05-18-03-44-09\n"
     ]
    },
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p3.16xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-19a9bbd68033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m               \u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjobname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m               \u001b[0mexperiment_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m               logs=True)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \"\"\"\n\u001b[1;32m   1448\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p3.16xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit."
     ]
    }
   ],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "exp_trial = Trial.create(experiment_name=experiment_name, \n",
    "                         trial_name=jobname)\n",
    "\n",
    "experiment_config={'ExperimentName': experiment_name,\n",
    "                   'TrialName': exp_trial.trial_name,\n",
    "                   'TrialComponentDisplayName': 'Training-Experiment-Trial'}\n",
    "\n",
    "estimator.fit(inputs=data_channels,\n",
    "              job_name=jobname,\n",
    "              experiment_config=experiment_config,\n",
    "              logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./imdb_data/model -p\n",
    "!aws s3 cp {estimator.model_data} ./imdb_data/model.tar.gz\n",
    "!tar -xzf ./imdb_data/model.tar.gz -C ./imdb_data/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model=tf.keras.models.load_model('./imdb_data/model/1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc=my_model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the deployment process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 2000\n",
    "maxlen = 100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "csv_test_dir_prefix = 'imdb_data/infernce'\n",
    "csv_test_filename = 'test.csv'\n",
    "csv_test_dir = os.path.join(os.getcwd(), csv_test_dir_prefix)\n",
    "os.makedirs(csv_test_dir, exist_ok=True)\n",
    "\n",
    "np.savetxt(os.path.join(csv_test_dir, csv_test_filename), \n",
    "           np.array(x_test, dtype=np.int32), fmt='%d', delimiter=\",\")\n",
    "\n",
    "test_data_s3prefix = f'{prefix}/infernce/csv_test'\n",
    "test_data_s3 = sess.upload_data(path=csv_test_dir, \n",
    "                                key_prefix=test_data_s3prefix)\n",
    "print(test_data_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Experiments and trials, you should see your training job as a trial in the list.\n",
    "training_job_name='imdb-tf-deepmodel-job-2022-04-30-19-10-49'\n",
    "\n",
    "# Once you have attached training_job_name and reload estimator, \n",
    "# you should see the history of the job printed in the output.\n",
    "\n",
    "estimator_deploy = TensorFlow.attach(training_job_name) # It gives details about the train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "\n",
    "# Creating a new trial for the experiment\n",
    "exp_trial = Trial.load(trial_name=training_job_name)\n",
    "\n",
    "experiment_config={\n",
    "    'ExperimentName': experiment_name,\n",
    "    'TrialName': exp_trial.trial_name,\n",
    "    'TrialComponentDisplayName': 'tf-model-inference-batchTransform'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "jobname = f'imdb-tf-deep-model-batach-transform-{exp_datetime}'\n",
    "\n",
    "s3_output_location = f's3://{bucket}/{prefix}/{jobname}'\n",
    "\n",
    "# Run SageMaker batch transform\n",
    "# Below method creates a Transformer object with the compute resource desired for the inference.\n",
    "# The max_payload argument allows us to control the size of each mini-batch \n",
    "# that SageMaker Batch Transform is splitting.\n",
    "transformer = estimator_deploy.transformer(instance_count=1, \n",
    "                                    instance_type='ml.c4.xlarge',\n",
    "                                    max_payload = 2,\n",
    "                                    accept = 'application/jsonlines',\n",
    "                                    output_path = s3_output_location,\n",
    "                                    assemble_with = 'Line')\n",
    "\n",
    "transformer.transform(test_data_s3, \n",
    "                      content_type='text/csv', \n",
    "                      split_type = 'Line', \n",
    "                      job_name = jobname,\n",
    "                      experiment_config = experiment_config)\n",
    "print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below code for error info\n",
    "#job_name = 'imdb-tf-deepmodel-job-2022-04-30-19-10-49'\n",
    "#sage = boto3.client('sagemaker')\n",
    "#sage.describe_training_job(TrainingJobName=job_name)['FailureReason']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SageMaker batch transform saves the results to the \n",
    "specified S3 location with .out appended to the input filename. \n",
    "We can access the full S3 path in transformer.output_path attribute. \n",
    "SageMaker uses TensorFlow Serving, a model serving framework developed by TensorFlow,\n",
    "for model serving, the model output is written in JSON format.\n",
    "The output has the sentiment probabilities in an array with predictions as the JSON key.\n",
    "We can inspect the batch transform results with the following code:\n",
    "\"\"\"\n",
    "output = transformer.output_path\n",
    "output_prefix = 'imdb_data/test_output'\n",
    "!mkdir -p {output_prefix}\n",
    "!aws s3 cp --recursive {output} {output_prefix}\n",
    "!head {output_prefix}/{csv_test_filename}.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "with open(f'{output_prefix}/{csv_test_filename}.out', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        print(line)\n",
    "        json_output = json.loads(line)\n",
    "        result = [float('%.3f'%(item)) for sublist in json_output['predictions'] \n",
    "                                       for item in sublist]\n",
    "        results += result\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(score):\n",
    "    return 'positive' if score > 0.5 else 'negative' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile(r'^[\\?\\s]+')\n",
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index=199\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "first_decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') \n",
    "                                 for i in x_test[data_index]])\n",
    "regex.sub('', first_decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Labeled sentiment for this review is {get_sentiment(y_test[data_index])}')\n",
    "print(f'Predicted sentiment is {get_sentiment(results[data_index])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fully managed mini-batching helps make inferences on a large dataset efficiently.\n",
    "You can use a separate SageMaker-managed compute infrastructure that is different from your notebook instance. You can easily run prediction with a cluster of instances for faster prediction.\n",
    "You only pay for the runtime of a batch transform job, even with a much larger compute cluster.\n",
    "You can schedule and kick off a model prediction independently in the cloud with SageMaker batch transform. It is not necessary to use a Python notebook in SageMaker Studio to start a prediction job.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hosting real-time endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Experiments and trials, you should see your training job as a trial in the list.\n",
    "training_job_name='imdb-tf-deepmodel-job-2022-04-30-19-10-49'\n",
    "\n",
    "# Once you have attached training_job_name and reload estimator, \n",
    "# you should see the history of the job printed in the output.\n",
    "\n",
    "estimator_real_time_deploy = TensorFlow.attach(training_job_name) # It gives details about the train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator_real_time_deploy.deploy(initial_instance_count=1, \n",
    "                             instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index=199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=predictor.predict(x_test[data_index])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(score):\n",
    "    return 'positive' if score > 0.5 else 'negative' \n",
    "\n",
    "import re\n",
    "\n",
    "regex = re.compile(r'^[\\?\\s]+')\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "first_decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') \n",
    "                                 for i in x_test[data_index]])\n",
    "regex.sub('', first_decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Labeled sentiment for this review is {get_sentiment(y_test[data_index])}')\n",
    "print(f'Predicted sentiment is {get_sentiment(prediction[\"predictions\"][0][0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor.predict(x_test[:5000]) # this would throw an error due to large volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing auto scaling \n",
    "sagemaker_client = sess.boto_session.client('sagemaker')\n",
    "autoscaling_client = sess.boto_session.client('application-autoscaling')\n",
    "\n",
    "endpoint_name = predictor.endpoint_name\n",
    "response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application autoscaling references the endpoint using string below\n",
    "resource_id=f'endpoint/{endpoint_name}/variant/AllTraffic' \n",
    "response = autoscaling_client.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=4\n",
    ")\n",
    "\n",
    "response = autoscaling_client.put_scaling_policy(\n",
    "    PolicyName='Invocations-ScalingPolicy',\n",
    "    ServiceNamespace='sagemaker', # The namespace of the AWS service that provides the resource. \n",
    "    ResourceId=resource_id, # Endpoint name \n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    PolicyType='TargetTrackingScaling', # Other options are 'StepScaling'|'ScheduledScaling'\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 4000.0, # The target value for the metric below.\n",
    "        'PredefinedMetricSpecification': {\n",
    "            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance', \n",
    "        },\n",
    "        'ScaleInCooldown': 600, \n",
    "        'ScaleOutCooldown': 300,\n",
    "        'DisableScaleIn': False # If true, scale-in is disabled.\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = autoscaling_client.describe_scaling_policies(\n",
    "    ServiceNamespace='sagemaker'\n",
    ")\n",
    "\n",
    "for i in response['ScalingPolicies']:\n",
    "    print('')\n",
    "    print(i['PolicyName'])\n",
    "    print('')\n",
    "    if('TargetTrackingScalingPolicyConfiguration' in i):\n",
    "        print(i['TargetTrackingScalingPolicyConfiguration']) \n",
    "    else:\n",
    "        print(i['StepScalingPolicyConfiguration'])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typically it's recommended to delete the endpoint to stop incurring cost\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hosting multi-model endpoints to save costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A multi-model endpoint is a type of real-time endpoint in SageMaker \n",
    "that allows multiple models to be deployed behind the same endpoint.\n",
    "Hosting models trained for 50 US states in 1 endpoint instead of 50, that's a 98% cost saving\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing instance type and autoscaling with load testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We use a Python load testing framework called locust to perform the load testing in SageMaker Studio.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'sagemaker-smdb-dataparallel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 400\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'imdb_data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "csv_test_dir = os.path.join(data_dir, 'csv-test')\n",
    "os.makedirs(csv_test_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(train_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(train_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(test_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(test_dir, 'y_test.npy'), y_test)\n",
    "np.savetxt(os.path.join(csv_test_dir, 'csv-test.csv'), \n",
    "           np.array(x_test[:100], dtype=np.int32), fmt='%d', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_s3_prefix = f'{prefix}/imdb_data/train'\n",
    "testdata_s3_prefix = f'{prefix}/imdb_data/test'\n",
    "\n",
    "train_s3 = sess.upload_data(path='./imdb_data/train/', key_prefix=traindata_s3_prefix)\n",
    "test_s3 = sess.upload_data(path='./imdb_data/test/', key_prefix=testdata_s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/smdp_tensorflow_sentiment.py\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "import argparse\n",
    "import codecs\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import smdistributed.dataparallel.tensorflow as sdp\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 400\n",
    "embedding_dims = 300\n",
    "filters = 256\n",
    "kernel_size = 3\n",
    "hidden_dims = 256\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--drop_out_rate', type=float, default=0.2)\n",
    "\n",
    "    # data directories\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "\n",
    "    # model directory /opt/ml/model default set by SageMaker\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "def get_train_data(train_dir, batch_size):\n",
    "    x_train = np.load(os.path.join(train_dir, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(train_dir, 'y_train.npy'))\n",
    "    print(f'x train {x_train.shape} y train {y_train.shape}')\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_test_data(test_dir):\n",
    "    x_test = np.load(os.path.join(test_dir, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join(test_dir, 'y_test.npy'))\n",
    "    print(f'x test {x_test.shape} y test {y_test.shape}')\n",
    "\n",
    "    return x_test, y_test\n",
    "\n",
    "\n",
    "def get_model(args):\n",
    "    embedding_layer = tf.keras.layers.Embedding(max_features,\n",
    "                                                embedding_dims,\n",
    "                                                input_length=maxlen)\n",
    "\n",
    "    sequence_input = tf.keras.Input(shape=(maxlen,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = tf.keras.layers.Dropout(args.drop_out_rate)(embedded_sequences)\n",
    "    x = tf.keras.layers.Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)(x)\n",
    "    x = tf.keras.layers.MaxPooling1D()(x)\n",
    "    x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "    x = tf.keras.layers.Dense(hidden_dims, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(args.drop_out_rate)(x)\n",
    "    preds = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(sequence_input, preds)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(train_dataset, args):\n",
    "    model = get_model(args)\n",
    "    \n",
    "    loss = tf.losses.BinaryCrossentropy(name = 'binary_crossentropy')\n",
    "    acc = tf.metrics.BinaryAccuracy(name = 'accuracy')\n",
    "    optimizer = tf.optimizers.Adam(learning_rate = args.learning_rate)\n",
    "    \n",
    "    @tf.function\n",
    "    def training_step(x_train, y_train, first_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            probs = model(x_train, training=True)\n",
    "            loss_value = loss(y_train, probs)\n",
    "            acc_value = acc(y_train, probs)\n",
    "\n",
    "        # SMDataParallel: Wrap tf.GradientTape with SMDataParallel's DistributedGradientTape\n",
    "        tape = sdp.DistributedGradientTape(tape, sparse_as_dense = True)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if first_batch:\n",
    "            print('first batch')\n",
    "            # SMDataParallel: Broadcast model and optimizer variables\n",
    "            sdp.broadcast_variables(model.variables, root_rank=0)\n",
    "            sdp.broadcast_variables(optimizer.variables(), root_rank=0)\n",
    "\n",
    "        # SMDataParallel: all_reduce call\n",
    "        loss_value = sdp.oob_allreduce(loss_value)  # Average the loss across workers\n",
    "        acc_value = sdp.oob_allreduce(acc_value)\n",
    " \n",
    "        return loss_value, acc_value\n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        for batch, (x_train, y_train) in enumerate(train_dataset.take(len(train_dataset)//sdp.size())):\n",
    "            is_first_batch = (epoch == 0) and (batch == 0)\n",
    "            loss_value, acc_value = training_step(x_train, y_train, is_first_batch)\n",
    "\n",
    "            if batch % 10 == 0 and sdp.rank() == 0:\n",
    "                print('Epoch #%d, Step #%d\\tLoss: %.6f, Acc: %.6f (batch_size=%d)' % (epoch, batch, loss_value, acc_value, len(y_train)))\n",
    "\n",
    "    # SMDataParallel: Save checkpoints only from master node.\n",
    "    if sdp.rank() == 0:\n",
    "        model.save(os.path.join(args.model_dir, '1'))\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args, _ = parse_args()\n",
    "\n",
    "    # initialize sagemaker data parallel (dist)\n",
    "    sdp.init()\n",
    "\n",
    "    # ping each GPU to a single smdistributed.dataparallel process with local_rank\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    if gpus:\n",
    "        tf.config.experimental.set_visible_devices(gpus[sdp.local_rank()], 'GPU')\n",
    "        \n",
    "    # scale the learning rate by number of workers\n",
    "    print('sdp.size() = %s' % sdp.size())\n",
    "    args.learning_rate = args.learning_rate * sdp.size()\n",
    "    \n",
    "    train_dataset = get_train_data(args.train, args.batch_size)\n",
    "\n",
    "    train(train_dataset, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "experiment_name = 'imdb-sentiment-analysis'\n",
    "\n",
    "try:\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=experiment_name, \n",
    "        description='Training a sentiment classification model using imdb dataset.')\n",
    "except ClientError as e:\n",
    "    print(f'{experiment_name} experiment already exists! Reusing the existing experiment.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "jobname = f'imdb-smdp-tf-{exp_datetime}'\n",
    "\n",
    "s3_output_location = f's3://{bucket}/{prefix}/{jobname}'\n",
    "code_dir = f's3://{bucket}/{prefix}/{jobname}'\n",
    "\n",
    "# SMDP supports ml.p4d.24xlarge, ml.p3dn.24xlarge, and ml.p3.16xlarge\n",
    "train_instance_type = 'ml.p3.16xlarge'\n",
    "hyperparameters = {'epochs': 30, 'batch_size': 512, \n",
    "                   'learning_rate': 0.001, 'drop_out_rate': 0.2}\n",
    "\n",
    "distribution = {'smdistributed': {'dataparallel': {'enabled': True}}}\n",
    "\n",
    "estimator = TensorFlow(source_dir='code',\n",
    "                       entry_point='smdp_tensorflow_sentiment.py',\n",
    "                       output_path=s3_output_location,\n",
    "                       code_location=code_dir,\n",
    "                       instance_type=train_instance_type,\n",
    "                       instance_count=1,\n",
    "                       enable_sagemaker_metrics=True,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       sagemaker_session=sess,\n",
    "                       role=role,\n",
    "                       framework_version='2.4',\n",
    "                       py_version='py37', \n",
    "                       distribution=distribution)\n",
    "\n",
    "data_channels = {'train':train_s3, 'test': test_s3}\n",
    "print(data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new trial for the experiment\n",
    "exp_trial = Trial.create(experiment_name=experiment_name, \n",
    "                         trial_name=jobname)\n",
    "\n",
    "experiment_config={'ExperimentName': experiment_name,\n",
    "                   'TrialName': exp_trial.trial_name,\n",
    "                   'TrialComponentDisplayName': 'Training-smdb'}\n",
    "\n",
    "estimator.fit(inputs=data_channels,\n",
    "              job_name=jobname,\n",
    "              experiment_config=experiment_config,\n",
    "              wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitoring model training and compute resources with SageMaker Debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SageMaker Debugger helps developers monitor the compute resource utilization, \n",
    "detect modeling-related issues, profile deep learning operations, \n",
    "and identify bottlenecks during the runtime of your training jobs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spot_training_checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'sagemaker-spottraining-checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data available locally.\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 400\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'imdb_data')\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "train_file = os.path.join(train_dir, 'x_train.npy')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "test_file = os.path.join(test_dir, 'x_test.npy')\n",
    "\n",
    "if not (os.path.isfile(train_file) and os.path.isfile(test_file)):\n",
    "    print('Data not available locally. Creating...')\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.preprocessing import sequence\n",
    "    from tensorflow.python.keras.datasets import imdb\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "    print(len(x_train), 'train sequences')\n",
    "    print(len(x_test), 'test sequences')\n",
    "\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "    x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print('x_test shape:', x_test.shape)\n",
    "    \n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    \n",
    "    np.save(os.path.join(train_dir, 'x_train.npy'), x_train)\n",
    "    np.save(os.path.join(train_dir, 'y_train.npy'), y_train)\n",
    "    np.save(os.path.join(test_dir, 'x_test.npy'), x_test)\n",
    "    np.save(os.path.join(test_dir, 'y_test.npy'), y_test)\n",
    "else:\n",
    "    print('Data available locally.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_s3_prefix = f'{prefix}/imdb_data/train'\n",
    "testdata_s3_prefix = f'{prefix}/imdb_data/test'\n",
    "\n",
    "train_s3 = sess.upload_data(path='./imdb_data/train/', key_prefix=traindata_s3_prefix)\n",
    "test_s3 = sess.upload_data(path='./imdb_data/test/', key_prefix=testdata_s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘code’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/tensorflow_sentiment_with_checkpoint.py\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "import argparse\n",
    "import codecs\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 400\n",
    "embedding_dims = 300\n",
    "filters = 256\n",
    "kernel_size = 3\n",
    "hidden_dims = 256\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--drop_out_rate', type=float, default=0.2)\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default='/opt/ml/checkpoints', \n",
    "                        help='Path where checkpoints will be saved.')\n",
    "\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "def save_history(path, history):\n",
    "    history_for_json = {}\n",
    "    # transform float values that aren't json-serializable\n",
    "    for key in list(history.history.keys()):\n",
    "        if type(history.history[key]) == np.ndarray:\n",
    "            history_for_json[key] == history.history[key].tolist()\n",
    "        elif type(history.history[key]) == list:\n",
    "            if type(history.history[key][0]) == np.float32 or type(history.history[key][0]) == np.float64:\n",
    "                history_for_json[key] = list(map(float, history.history[key]))\n",
    "\n",
    "    with codecs.open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(history_for_json, f, separators=(',', ':'), sort_keys=True, indent=4) \n",
    "\n",
    "\n",
    "def get_train_data(train_dir):\n",
    "    x_train = np.load(os.path.join(train_dir, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(train_dir, 'y_train.npy'))\n",
    "    print(f'x train {x_train.shape} y train {y_train.shape}')\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "def get_test_data(test_dir):\n",
    "    x_test = np.load(os.path.join(test_dir, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join(test_dir, 'y_test.npy'))\n",
    "    print(f'x test {x_test.shape} y test {y_test.shape}')\n",
    "\n",
    "    return x_test, y_test\n",
    "\n",
    "\n",
    "def get_model(args):\n",
    "    embedding_layer = tf.keras.layers.Embedding(max_features,\n",
    "                                                embedding_dims,\n",
    "                                                input_length=maxlen)\n",
    "\n",
    "    sequence_input = tf.keras.Input(shape=(maxlen,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = tf.keras.layers.Dropout(args.drop_out_rate)(embedded_sequences)\n",
    "    x = tf.keras.layers.Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)(x)\n",
    "    x = tf.keras.layers.MaxPooling1D()(x)\n",
    "    x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "    x = tf.keras.layers.Dense(hidden_dims, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(args.drop_out_rate)(x)\n",
    "    preds = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(sequence_input, preds)\n",
    "    optimizer = tf.keras.optimizers.Adam(args.learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model_from_checkpoints(checkpoint_dir):\n",
    "    checkpoint_files = [file for file in os.listdir(checkpoint_dir) if file.endswith('.' + 'h5')]\n",
    "    print('------------------------------------------------------')\n",
    "    print(f'Available checkpoint files: {checkpoint_files}')\n",
    "    epoch_numbers = [re.search('(\\.*([1-9]|[1-9][0-9]|[1-9][0-9][0-9]))(?=\\.)', file).group() \n",
    "                     for file in checkpoint_files]\n",
    "      \n",
    "    max_epoch_number = max(epoch_numbers)\n",
    "    max_epoch_index = epoch_numbers.index(max_epoch_number)\n",
    "    max_epoch_filename = checkpoint_files[max_epoch_index]\n",
    "\n",
    "    print(f'Latest epoch checkpoint file name: {max_epoch_filename}')\n",
    "    print('Resuming training from epoch: {}'.format(int(max_epoch_number)+1))\n",
    "    print('------------------------------------------------------')\n",
    "    \n",
    "    resumed_model_from_checkpoints = tf.keras.models.load_model(f'{checkpoint_dir}/{max_epoch_filename}')\n",
    "    return resumed_model_from_checkpoints, int(max_epoch_number)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args, _ = parse_args()\n",
    "    print(args)\n",
    "\n",
    "    if os.path.isdir(args.checkpoint_dir):\n",
    "        print(f'Checkpointing directory {args.checkpoint_dir} exists.')\n",
    "    else:\n",
    "        print(f'Creating Checkpointing directory {args.checkpoint_dir}.')\n",
    "        os.mkdir(args.checkpoint_dir)\n",
    "        \n",
    "    x_train, y_train = get_train_data(args.train)\n",
    "    x_test, y_test = get_test_data(args.test)\n",
    "\n",
    "    # Load model\n",
    "    if not os.listdir(args.checkpoint_dir):\n",
    "        model = get_model(args)\n",
    "        initial_epoch_number = 0\n",
    "    else:    \n",
    "        model, initial_epoch_number = load_model_from_checkpoints(args.checkpoint_dir)\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.ModelCheckpoint(args.checkpoint_dir + '/checkpoint-{epoch}.h5')]\n",
    "    \n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=args.batch_size,\n",
    "                        epochs=args.epochs,\n",
    "                        initial_epoch=initial_epoch_number,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    save_history(args.model_dir + '/history.p', history)\n",
    "    \n",
    "    # create a TensorFlow SavedModel for deployment to a SageMaker endpoint with TensorFlow Serving\n",
    "    model.save(args.model_dir + '/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://sagemaker-us-east-1-104877823522/sagemaker-spottraining-checkpoint/imdb_data/train', 'test': 's3://sagemaker-us-east-1-104877823522/sagemaker-spottraining-checkpoint/imdb_data/test'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "jobname = f'imdb-tf-spot-{exp_datetime}'\n",
    "\n",
    "s3_output_location = f's3://{bucket}/{prefix}'\n",
    "code_dir = f's3://{bucket}/{prefix}'\n",
    "\n",
    "train_instance_type = 'ml.c2.xlarge'\n",
    "hyperparameters = {'epochs': 20, 'batch_size': 256, 'learning_rate': 0.01, 'drop_out_rate': 0.2}\n",
    "\n",
    "use_spot_instances = True\n",
    "max_run = 3600\n",
    "max_wait = 3600\n",
    "\n",
    "checkpoint_suffix = str(uuid.uuid4())[:8]\n",
    "checkpoint_suffix = '02fa28a1'\n",
    "checkpoint_s3_uri = f's3://{bucket}/{prefix}/checkpoint-{checkpoint_suffix}'\n",
    "checkpoint_local_path = '/opt/ml/checkpoints/'\n",
    "model_local_path = '/opt/ml/model'\n",
    "\n",
    "estimator = TensorFlow(source_dir='code',\n",
    "                       entry_point='tensorflow_sentiment_with_checkpoint.py',\n",
    "                       output_path=s3_output_location,\n",
    "                       model_dir=model_local_path,\n",
    "                       code_location=code_dir,\n",
    "                       instance_type=train_instance_type,\n",
    "                       instance_count=1,\n",
    "                       enable_sagemaker_metrics=True,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=role,\n",
    "                       framework_version='2.1',\n",
    "                       py_version='py3',\n",
    "                       use_spot_instances=use_spot_instances,\n",
    "                       checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "                       max_run=max_run,\n",
    "                       max_wait=max_wait,\n",
    "                       debugger_hook_config=False)\n",
    "\n",
    "data_channels = {'train':train_s3, 'test': test_s3}\n",
    "print(data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new trial for the experiment\n",
    "exp_trial = Trial.create(experiment_name=experiment_name, \n",
    "                         trial_name=jobname)\n",
    "\n",
    "experiment_config={'ExperimentName': experiment_name,\n",
    "                   'TrialName': exp_trial.trial_name,\n",
    "                   'TrialComponentDisplayName': 'Training'}\n",
    "\n",
    "estimator.fit(inputs=data_channels,\n",
    "              job_name=jobname,\n",
    "              experiment_config=experiment_config,\n",
    "              wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.7/site-packages (21.1.2)\n",
      "Collecting pip\n",
      "  Using cached pip-22.0.4-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.1.2\n",
      "    Uninstalling pip-21.1.2:\n",
      "      Successfully uninstalled pip-21.1.2\n",
      "Successfully installed pip-22.0.4\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip \n",
    "!pip install -q sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "local_prefix = 'monitoring'\n",
    "prefix = f'sagemaker-monitoring/{local_prefix}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import json, os, re, uuid\n",
    "from time import sleep, gmtime, strftime\n",
    "from threading import Thread\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.processing import ProcessingJob\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names taken from https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.names\n",
    "columns = ['Sex', 'Length', 'Diameter', 'Height', 'WholeWeight', \n",
    "           'ShuckedWeight', 'VisceraWeight', 'ShellWeight', 'Rings']\n",
    "df=pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data', \n",
    "               names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df.copy()\n",
    "# Convert Rings to float so that model prediction (regression) and \n",
    "# the ground truth are both of float type for model monitor to work with\n",
    "df_processed['Rings']=df_processed['Rings'].astype(float)\n",
    "df_processed['Sex'] = df_processed['Sex'].replace(to_replace=['M', 'F', 'I'], \n",
    "                                                  value=[2., 1., 0.])\n",
    "# moving the target Rings to the first so that we can train with XGBoost.\n",
    "columns=['Rings', 'Sex', 'Length', 'Diameter', 'Height', 'WholeWeight', \n",
    "         'ShuckedWeight', 'VisceraWeight', 'ShellWeight']\n",
    "df_processed = df_processed[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_build, df_test = train_test_split(df_processed, test_size=0.1, random_state=42, \n",
    "                                     shuffle=True, stratify=df_processed['Sex'])\n",
    "df_train, df_val = train_test_split(df_build, test_size=1/9., random_state=42, \n",
    "                                    shuffle=True, stratify=df_build['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_no_target = ['Sex', 'Length', 'Diameter', 'Height', 'WholeWeight', \n",
    "                     'ShuckedWeight', 'VisceraWeight', 'ShellWeight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(local_prefix, exist_ok=True)\n",
    "df_train.to_csv(f'./{local_prefix}/abalone_train.csv', index=False)\n",
    "df_val.to_csv(f'./{local_prefix}/abalone_val.csv', index=False)\n",
    "df_test.to_csv(f'./{local_prefix}/abalone_test.csv', index=False)\n",
    "\n",
    "desired_s3_uri = f's3://{bucket}/{prefix}/data'\n",
    "train_data_s3 = sagemaker.s3.S3Uploader.upload(local_path=f'./{local_prefix}/abalone_train.csv',\n",
    "                                               desired_s3_uri=desired_s3_uri,\n",
    "                                               sagemaker_session=sess)\n",
    "val_data_s3 = sagemaker.s3.S3Uploader.upload(local_path=f'./{local_prefix}/abalone_val.csv',\n",
    "                                             desired_s3_uri=desired_s3_uri,\n",
    "                                             sagemaker_session=sess)\n",
    "test_data_s3 = sagemaker.s3.S3Uploader.upload(local_path=f'./{local_prefix}/abalone_test.csv',\n",
    "                                              desired_s3_uri=desired_s3_uri,\n",
    "                                              sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: abalone-xgb-2022-05-03-23-40-37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-03 23:40:37 Starting - Starting the training job...\n",
      "2022-05-03 23:40:55 Starting - Preparing the instances for trainingProfilerReport-1651621237: InProgress\n",
      "......\n",
      "2022-05-03 23:42:05 Downloading - Downloading input data...\n",
      "2022-05-03 23:42:30 Training - Downloading the training image.....\n",
      "2022-05-03 23:43:37 Uploading - Uploading generated training model\n",
      "2022-05-03 23:43:37 Completed - Training job completed\n",
      "\u001b[34m[2022-05-03 23:43:19.847 ip-10-2-233-184.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-05-03:23:43:19:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2022-05-03:23:43:19:INFO] Failed to parse hyperparameter objective value reg:squarederror to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2022-05-03:23:43:19:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2022-05-03:23:43:19:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2022-05-03:23:43:19:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2022-05-03:23:43:19:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2022-05-03:23:43:19:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2022-05-03:23:43:19:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2022-05-03:23:43:19:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2022-05-03:23:43:19:INFO] Train matrix has 3342 rows and 8 columns\u001b[0m\n",
      "\u001b[34m[2022-05-03:23:43:19:INFO] Validation matrix has 419 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:7.19031#011validation-rmse:7.09120\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:5.26818#011validation-rmse:5.18933\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:3.97344#011validation-rmse:3.93413\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:3.12392#011validation-rmse:3.13397\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:2.58449#011validation-rmse:2.63431\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:2.25421#011validation-rmse:2.34095\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:2.05587#011validation-rmse:2.17463\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:1.94742#011validation-rmse:2.08342\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:1.85116#011validation-rmse:2.02335\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:1.77368#011validation-rmse:1.99360\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:1.70419#011validation-rmse:1.98143\u001b[0m\n",
      "\u001b[34m[11]#011train-rmse:1.67668#011validation-rmse:1.97425\u001b[0m\n",
      "\u001b[34m[12]#011train-rmse:1.65429#011validation-rmse:1.96650\u001b[0m\n",
      "\u001b[34m[13]#011train-rmse:1.63129#011validation-rmse:1.97654\u001b[0m\n",
      "\u001b[34m[14]#011train-rmse:1.61463#011validation-rmse:1.97086\u001b[0m\n",
      "\u001b[34m[15]#011train-rmse:1.60280#011validation-rmse:1.97469\u001b[0m\n",
      "\u001b[34m[16]#011train-rmse:1.59273#011validation-rmse:1.97433\u001b[0m\n",
      "\u001b[34m[17]#011train-rmse:1.57404#011validation-rmse:1.97569\u001b[0m\n",
      "\u001b[34m[18]#011train-rmse:1.56728#011validation-rmse:1.97850\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:1.55858#011validation-rmse:1.98082\u001b[0m\n",
      "Training seconds: 92\n",
      "Billable seconds: 92\n"
     ]
    }
   ],
   "source": [
    "image = image_uris.retrieve(region=region, framework='xgboost', version='1.3-1')\n",
    "\n",
    "exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "jobname = f'abalone-xgb-{exp_datetime}'\n",
    "\n",
    "experiment_name = 'abalone-age-prediction'\n",
    "\n",
    "try:\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=experiment_name, \n",
    "        description='Predicting age for abalone based on physical measurements.')\n",
    "except ClientError as e:\n",
    "    print(f'{experiment_name} experiment already exists! Reusing the existing experiment.')\n",
    "    \n",
    "# Creating a new trial for the experiment\n",
    "exp_trial = Trial.create(experiment_name=experiment_name, \n",
    "                         trial_name=jobname)\n",
    "\n",
    "experiment_config={'ExperimentName': experiment_name,\n",
    "                   'TrialName': exp_trial.trial_name,\n",
    "                   'TrialComponentDisplayName': 'Training'}\n",
    "\n",
    "train_s3_output = f's3://{bucket}/{prefix}/abalone_data/training'\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(image,\n",
    "                                    role,\n",
    "                                    instance_type='ml.m5.xlarge',\n",
    "                                    instance_count=1,\n",
    "                                    output_path=train_s3_output,\n",
    "                                    enable_sagemaker_metrics=True,\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "xgb.set_hyperparameters(objective='reg:squarederror', num_round=20)\n",
    "\n",
    "train_input = sagemaker.inputs.TrainingInput(s3_data=train_data_s3, \n",
    "                                             content_type='csv')\n",
    "val_input = sagemaker.inputs.TrainingInput(s3_data=val_data_s3, \n",
    "                                           content_type='csv')\n",
    "data_channels={'train': train_input, 'validation': val_input}\n",
    "\n",
    "xgb.fit(inputs=data_channels, \n",
    "        job_name=jobname, \n",
    "        experiment_config=experiment_config, \n",
    "        wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capture path: s3://sagemaker-us-east-1-104877823522/sagemaker-monitoring/monitoring/datacapture\n",
      "Ground truth path: s3://sagemaker-us-east-1-104877823522/sagemaker-monitoring/monitoring/ground-truth-data/2022-05-03-23-40-37\n",
      "Report path: s3://sagemaker-us-east-1-104877823522/sagemaker-monitoring/monitoring/reports\n"
     ]
    }
   ],
   "source": [
    "##S3 prefixes\n",
    "data_capture_prefix = f'{prefix}/datacapture'\n",
    "s3_capture_upload_path = f's3://{bucket}/{data_capture_prefix}'\n",
    "\n",
    "ground_truth_upload_path = f's3://{bucket}/{prefix}/ground-truth-data/{exp_datetime}'\n",
    "\n",
    "reports_prefix = f'{prefix}/reports'\n",
    "s3_report_path = f's3://{bucket}/{reports_prefix}'\n",
    "\n",
    "print(f'Capture path: {s3_capture_upload_path}')\n",
    "print(f'Ground truth path: {ground_truth_upload_path}')\n",
    "print(f'Report path: {s3_report_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "data_capture_config = DataCaptureConfig(enable_capture=True, \n",
    "                                        sampling_percentage=100, \n",
    "                                        destination_s3_uri=s3_capture_upload_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2022-05-03-23-51-53-287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointName: abalone-xgb-2022-05-03-23-40-37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint with name abalone-xgb-2022-05-03-23-40-37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = f'abalone-xgb-{exp_datetime}'\n",
    "print(f'EndpointName: {endpoint_name}')\n",
    "\n",
    "predictor = xgb.deploy(initial_instance_count=1,\n",
    "                       instance_type='ml.m5.large',\n",
    "                       endpoint_name=endpoint_name,\n",
    "                       serializer=CSVSerializer(),\n",
    "                       data_capture_config=data_capture_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_np = Predictor(endpoint_name=endpoint_name, \n",
    "                         sagemaker_session=sess,\n",
    "                         serializer=CSVSerializer(),\n",
    "                         deserializer=CSVDeserializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=predictor_np.predict(df_val[columns_no_target].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_f = [float(i) for i in pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rings</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>WholeWeight</th>\n",
       "      <th>ShuckedWeight</th>\n",
       "      <th>VisceraWeight</th>\n",
       "      <th>ShellWeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2779</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.9235</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>0.1730</td>\n",
       "      <td>0.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.0475</td>\n",
       "      <td>0.3775</td>\n",
       "      <td>0.1705</td>\n",
       "      <td>0.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2171</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.8115</td>\n",
       "      <td>0.4035</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.6440</td>\n",
       "      <td>0.2850</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>0.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.240</td>\n",
       "      <td>2.1995</td>\n",
       "      <td>0.7245</td>\n",
       "      <td>0.4650</td>\n",
       "      <td>0.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4084</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>0.5060</td>\n",
       "      <td>0.2485</td>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.155</td>\n",
       "      <td>1.0140</td>\n",
       "      <td>0.4510</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.4555</td>\n",
       "      <td>0.1770</td>\n",
       "      <td>0.0965</td>\n",
       "      <td>0.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2404</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.8435</td>\n",
       "      <td>0.3090</td>\n",
       "      <td>0.1905</td>\n",
       "      <td>0.300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rings  Sex  Length  Diameter  Height  WholeWeight  ShuckedWeight  \\\n",
       "2779    9.0  2.0   0.590     0.470   0.145       0.9235         0.4545   \n",
       "429    18.0  1.0   0.575     0.450   0.170       1.0475         0.3775   \n",
       "2171    6.0  0.0   0.190     0.130   0.030       0.0295         0.0155   \n",
       "3449    7.0  2.0   0.520     0.395   0.125       0.8115         0.4035   \n",
       "196    11.0  1.0   0.505     0.410   0.150       0.6440         0.2850   \n",
       "...     ...  ...     ...       ...     ...          ...            ...   \n",
       "2161   17.0  1.0   0.715     0.565   0.240       2.1995         0.7245   \n",
       "4084   10.0  1.0   0.575     0.480   0.170       1.1000         0.5060   \n",
       "1353   11.0  2.0   0.600     0.480   0.155       1.0140         0.4510   \n",
       "2121    9.0  0.0   0.475     0.360   0.110       0.4555         0.1770   \n",
       "2404   15.0  0.0   0.555     0.455   0.170       0.8435         0.3090   \n",
       "\n",
       "      VisceraWeight  ShellWeight  \n",
       "2779         0.1730        0.254  \n",
       "429          0.1705        0.385  \n",
       "2171         0.0150        0.010  \n",
       "3449         0.1660        0.200  \n",
       "196          0.1450        0.210  \n",
       "...             ...          ...  \n",
       "2161         0.4650        0.885  \n",
       "4084         0.2485        0.310  \n",
       "1353         0.1885        0.325  \n",
       "2121         0.0965        0.145  \n",
       "2404         0.1905        0.300  \n",
       "\n",
       "[418 rows x 9 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val\n",
    "#pred_f\n",
    "#df_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
